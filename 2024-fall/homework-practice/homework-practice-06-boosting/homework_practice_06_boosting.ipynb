{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7EowSrjCoUyN"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "# Практическое задание 6. Бустинговое\n",
        "\n",
        "## Общая информация\n",
        "\n",
        "Дата выдачи: 12.12.2024\n",
        "\n",
        "Мягкий дедлайн: 22.12.2024 23:59 MSK\n",
        "\n",
        "Жёсткий дедлайн: 22.12.2024 23:59 MSK\n",
        "\n",
        "## Оценивание и штрафы\n",
        "\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "## Формат сдачи\n",
        "Задания сдаются через систему anytask. Посылка должна содержать:\n",
        "* Ноутбук homework-practice-06-Username.ipynb\n",
        "\n",
        "Username — ваша фамилия на латинице"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KrDlbgTtoUyP"
      },
      "source": [
        "## О задании\n",
        "\n",
        "В этом задании вам предстоит вручную запрограммировать один из самых мощных алгоритмов машинного обучения — бустинг. Однако с большой силой приходит и большая ответственность, так что заодно научимся, как его правильно готовить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yWktQHZQoUyQ"
      },
      "outputs": [],
      "source": [
        "from warnings import filterwarnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.sparse import load_npz\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "n0zcZqIZoUyR"
      },
      "outputs": [],
      "source": [
        "X = load_npz('x.npz')\n",
        "y = np.load('y.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zOIxzg1joUyR"
      },
      "source": [
        "Разделим на обучающую, валидационную и тестовую выборки (оставьте `random_state=1337` для воспроизводимости)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "D_DAsR6roUyS"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)\n",
        "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=1337)\n",
        "\n",
        "X_train.shape, X_valid.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "shCFxbqroUyS"
      },
      "source": [
        "---\n",
        "\n",
        "## Задание 1. Базовый градиентный бустинг (4 балла + 0.5 бонус)\n",
        "\n",
        "Первая часть посвящена реализации собственного градиентного бустинга. Обращаем внимание, что пользоваться готовыми реализациями <font color='lightcoral'>**нельзя**</font>, если специально не оговорено. На все <font color='plum'>**вопросы**</font> должен быть <font color='plum'>**ответ**</font>. В заданиях есть референсы для параметров, которые нужно сделать, на случай, если объяснение из ноутбука непонятно, тогда советуем обратиться к соответствующей документации\n",
        "\n",
        "Вам нужно дописать код в файлике `boosting.py`. Для вас уже подготовлен шаблон класса `Boosting`, вы можете менять его по своему усмотрению.\n",
        "\n",
        "### Инструкции для функций:\n",
        "\n",
        "#### `__init__`\n",
        "\n",
        "Обязательные параметры:\n",
        " - `base_model_class` - класс базовой модели нашего бустинга\n",
        " - `base_model_params` - словарь с гиперпараметрами для базовой модели\n",
        " - `n_estimators` - какое количество базовых моделей нужно обучить\n",
        " - `learning_rate` - темп обучения, должен быть из полуинтервала $(0, 1]$\n",
        "\n",
        "#### `fit`\n",
        "\n",
        "В `fit` приходит выборка, на которой мы обучаем новые базовые модели\n",
        "\n",
        "Сначала нам нужно сделать какую-то нулевую модель, сделать предсказания (в шаблоне это нулевая модель, соответственно предсказания это просто `np.zeros`). После этого нужно обучить `n_estimators` базовых моделей (как и на что обучаются базовые модели смотрите в лекциях и семинарах). После каждой обученной базовой модели мы должны обновить текущие предсказания, посчитать ошибку на выборке (используем `loss_fn` для этого) и найти новую оптимальную гамму\n",
        "\n",
        "После всего цикла обучения надо нарисовать график (если `plot=True`).\n",
        "\n",
        "#### `partial_fit`\n",
        "\n",
        "В `partial_fit` приходит обучающая выборка, на которую обучается новая базовая модель\n",
        "\n",
        "#### `predict_proba`\n",
        "\n",
        "В `predict_proba` приходит любая выборка, нужно предсказать вероятности для неё. Суммируем предсказания базовых моделей на этой выборке (не забываем про гаммы) и накидываем сигмоиду"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5buucrG2oUyT"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3Xm_ImHDoUyT"
      },
      "outputs": [],
      "source": [
        "%autoreload 2\n",
        "\n",
        "from boosting import Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "y7UHXk1UoUyT"
      },
      "source": [
        "### Проверка кода\n",
        "\n",
        "У автора задания всё учится около одной секунды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ahY_-dDAoUyT"
      },
      "outputs": [],
      "source": [
        "boosting = Boosting(plot=True, n_estimators=100)\n",
        "\n",
        "boosting.fit(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "assert len(boosting.models) == boosting.n_estimators\n",
        "assert len(boosting.gammas) == boosting.n_estimators\n",
        "\n",
        "assert boosting.predict_proba(X_test).shape == (X_test.shape[0], 2)\n",
        "\n",
        "print(f'Train ROC-AUC {boosting.score(X_train, y_train):.4f}')\n",
        "print(f'Valid ROC-AUC {boosting.score(X_valid, y_valid):.4f}')\n",
        "print(f'Test ROC-AUC {boosting.score(X_test, y_test):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYluiiWooUyU"
      },
      "source": [
        "### Бонус. Бустинг линейных моделей (0.5 балла)\n",
        "\n",
        "**<font color='plum'>Вопрос:** что произойдет при ансамблировании линейных моделей? Имеет ли это смысл?</font>\n",
        "\n",
        "<font color='plum'>**Ответ:** ...</font>\n",
        "\n",
        "Давайте убедимся. Обучите бустинг, где в качестве базовой модели будет линейная регрессия. Обязательно сохраните историю функции потерь и зафиксируйте время обучения. Можно взять уже готовый из <font color='lightblue'>XGBoost</font>, но будьте готовы разобраться с тем, как оттуда достать историю обучения\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| `XGBClassifier(booster=\"gblinear\")` | - | - |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HXOSyGh6oUyU"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaIWJMJvoUyU"
      },
      "source": [
        "Теперь возьмите `sklearn.linear_model.SGDClassifier` иобучите на тех же данных. Не забудьте вытащить историю функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icEvYeADoUyU"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLHf2JVjoUyU"
      },
      "source": [
        "Покажите на одном графике изменение функции потерь для бустинга и SGD классификатора. Сравните следующие параметры у бустинга и линейной модели:\n",
        "\n",
        "- время обучения\n",
        "- число итераций до сходимости\n",
        "- итоговое значение функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sxDPTVxoUyU"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFGyl7M-oUyU"
      },
      "source": [
        "<font color='plum'>**Вопрос:** Какой вывод вы можете сделать?</font>\n",
        "\n",
        "<font color='plum'>**Ответ:** ... </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ua0qKX9oUyV"
      },
      "source": [
        "---\n",
        "\n",
        "## Задание 2. Имплементация гиперпараметров (2.5 балла + 3 бонус)\n",
        "\n",
        "Бустинг, как метод агрегации, предлагает несколько вариантов регуляризации безотносительно базовой модели. В этом пункте предлагается имплементировать часть из них. При необходимости можете обратиться к реализациям в вашей любимой библиотеке, там могут быть дополнительные пояснения и/или ссылки на оригинальные статьи\n",
        "\n",
        "<font color='lightcoral'>**Важно!**</font> После добавления каждого параметра проверьте, как это повлияет на качество. Специально крутить параметры не нужно, но важно увидеть, что перформанс модели изменился, в худшую или в лучшую сторону, лучше всего это видно по графику обучения\n",
        "\n",
        "### 2.1. Бутстрап (0.5 балла)\n",
        "Как известно, при обучении базовых алгоритмов в беггинге, используется бутстрапированная выборка. Звучит хайпово, почему бы не попробовать сделать то же самое в бустинге?\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| `subsample`, - | `subsample`, `bagging`| `subsample`, `bootstrap_type`|\n",
        "\n",
        "Вам нужно реализовать параметры:\n",
        " - `subsample: float | int = 0.6` - доля или число объектов, на которой будет обучаться базовая модель (какую часть составляет бутстрапная выборка от исходной обучающей). Соответствует бутстрапу типа `Bernoulli`. Каждый объект либо входит в подвыборку, либо нет\n",
        " - `bagging_temperature: float | int = 1.0` - веса объектов, которые попадают в выборку. Соответствует бутстрапу типа `Bayesian`. Каждый объект имеет определенный вес $t$, на который впоследствии домножается по формуле $w = -log(X^t)$, где $X \\sim {U[0, 1]}$, после чего попадает в выборку. Таким образом можно сделать симуляцию повторений\n",
        " - `bootstrap_type: str | None = 'Bernoulli'` - тип бутстрапа\n",
        "\n",
        "<font color='plum'>**Вопрос:** как бутстрап может помочь в обучении с точки зрения смещения, разброса и вообще?</font>\n",
        "\n",
        "<font color='plum'>**Ответ:** ... </font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ],
      "metadata": {
        "id": "01RerBA03Pc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP0tgrFZoUyV"
      },
      "source": [
        "### Бонус. GOSS (0.5 балла)\n",
        "\n",
        "Заметим, что во время обучения не все объекты одинаково важны. Один из вариантов это исправить - ввести какие-то веса на объекты, однако можно пойти чуть хитрее. В <font color='lightgreen'>LightGBM</font> придумали подход [Gradient-based One-side Sampling](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf)\n",
        "\n",
        "Давайте оценивать важность объектов в отдельный момент обучения нового дерева. Будем считать, какой антиградиент привносит каждый объект. Далее разобъем градиенты на 2 группы. Объекты с большими градиентами берутся все, с маленькими - только часть, по аналогии с бутстрапом, с отрицательными - выкидываются. И не забываем домножить на фактор, чтобы учесть выкинутые объекты. Подробнее в статье\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| - | `bagging=goss`| `bootstrap_type=MVS`|\n",
        "\n",
        "Параметры, которые нужно сделать:\n",
        "\n",
        "- `goss: bool | None = True` - GOSS бустинг или нет\n",
        "- `goss_k: float | int = 0.2` - доля объектов, градиенты которых будем считать большими. Все остальное - маленькими\n",
        "- `subsample: float | int = 0.3` - параметры сэмплинга для маленьких градиентов из пункта 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM4YSnxCoUyV"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FG0J-cvoUyV"
      },
      "source": [
        "### 2.2. Работа с признаками (1 балл)\n",
        "\n",
        "1. Помимо выбора случайных объектов, есть идея выбирать случайное подмножество признаков, заложенная в Random Forest. Мотивация это делать точно та же самая, что и в бутстрапировании, за это будет отвечать `rsm`\n",
        "\n",
        "2. С другой стороны, вместо того, чтобы выкидывать признаки, можно попробовать проредить их множество значений, применив бинаризацию. В идеале это стоит делать с датасетом до обучения модели, но не обязательно, можно, например, на этапе `fit`, а в `partial_fit` приходит уже квантизованный датасет. Для этого существует множество алгоритмов, мы предлагаем сделать два наиболее простых:\n",
        "\n",
        "- $\\text{Uniform}$. Значения признака $[\\min f_i, \\max f_i]$ отображаются в `nbins` отрезков одинаковой длины, которые полностью покрывают интервал\n",
        "- $\\text{Quantile}$. Тот же самый отрезок сортируется и бьется на `nbins` квантилей от 0 до 1\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| `colsample_bytree`, `tree_method` | `colsample_bytree`, `max_bin`| `rsm`, `quantize(...)`|\n",
        "\n",
        "Нужно добавить параметры:\n",
        "- `rsm: float | int = 0.6` - доля или число признаков, участвующих в обучении модели. Каждый признак либо входит, либо не входит в пул, на котором обучается базовый алгоритм, по аналогии с бутстрапом Бернулли\n",
        "- `quantization_type: str | None = True` - тип квантизации, если она есть\n",
        "- `nbins: int = 255` - число бинов для квантизации, можно оставить дефолтное значение. Игнорируем, если тип квантизации не указан\n",
        "\n",
        "<font color='plum'>**Вопрос**: как квантизация поможет в обработке выбросов и нанов?</font>\n",
        "\n",
        "<font color='plum'>**Ответ**: ... </font>\n",
        "\n",
        "<font color='plum'>**Вопрос**: можно ли просто заполнить наны каким-то числом? Имеет ли значение, что это за число (-1, -100, -1e32)?</font>\n",
        "\n",
        "<font color='plum'>**Ответ**: ... </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_sfjI1poUyV"
      },
      "source": [
        "### Бонус. Квантизация (1 балл)\n",
        "\n",
        "В текущих имплементациях бустинга используются более сложные методы квантизации. К сожалению, способы выше страдают либо от неравноправности бинов - в каких-то объектов больше, в каких-то меньше, либо от того, что целевая переменная от бинов зависит опосредованно. Предлагается это исправить следующими способами:\n",
        "\n",
        "1. $\\text{MinEntropy}$. Это [один из методов](https://catboost.ai/docs/en/concepts/quantization), который используется для бинаризации в <font color='palegoldenrod'>CatBoost</font>, остальные на него очень похожи. Будем жадно набирать бины так, чтобы энтропия объектов внутри ($\\sum_{i \\in \\text{bin}} x_i\\log(x_i)$) была как можно меньше. Сделать такое разбиение руками непросто, но вам эта задача должна что-то напоминать\n",
        "   \n",
        "2. $\\text{PiecewiseEncoding}$. Это относительно свежий метод для [табличных трансформеров](https://arxiv.org/pdf/2203.05556). Суть заключается в том, что бины для квантизации будем брать не с потолка, а именно те, что нужны для разбиения таргета. (В статье делается кодирование в вектор длины $T$, где $T$ число бинов, в нашем случае это излишне, бинов можно оставить столько же, но взять лмшь закодированную по формуле часть)\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| - | - | `feature_border_type=\"MinEntropy\"` |\n",
        "\n",
        "Нужно добавить опцию:\n",
        "\n",
        "- `quantization_type: str | None = 'MinEntropy'` - какую квантизацию используем\n",
        "\n",
        "Реализуйте оба способа подсчета (или один, но тогда балл неполный) и сравните, удалось ли улучшить качество?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4l96Q1OoUyV"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVVJUN_yoUyV"
      },
      "source": [
        "### 2.3. Важность признаков (0.5 балла)\n",
        "\n",
        "Поскольку наша базовая модель - это дерево из `sklearn`, мы можем вычислить важность признака отдельно для каждого дерева и усреднить (воспользуйтесь `feature_importances_` у `DecisionTreeRegressor`), после этого нормировать значения, чтобы они суммировались в единицу (обратите внимание, что они должны быть неотрицательными - иначе вы что-то сделали не так). Разумеется, способ не единственный, если хочется, посмотрите в документации\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| `model.get_score()` | `lightgbm.importance()` | `model.get_feature_importance()` |\n",
        "\n",
        "Допишите в вашей реализации бустинга атрибут `feature_importances_` чтобы она возвращала описанные выше важности признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvJbIISHoUyV"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojXppDXEoUyV"
      },
      "source": [
        "Покажите, какие признаки оказались самыми важными"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0mRuav_oUyV"
      },
      "source": [
        "### 2.4. Борьба с переобучением (0.5 балла)\n",
        "\n",
        "Как известно, бустинги бывают склонны к переобучению. Чтобы этого не допустить, можно контролировать процесс обучения и проверять критерий останова на валидации. Реализуйте такую процедуру, не забудьте, что всю предобработку, что вы делали для трейна, нужно будет применить и к валидационной выборке\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| <td colspan=3>`early_stopping_rounds`, `eval_set` |\n",
        "\n",
        "Добавьте параметры:\n",
        "- `early_stopping_rounds: int | None = 10` - число раундов для критерия останова. Если качество ухудшается на протяжении всех раундов, останавливаем обучение\n",
        "- `eval_set: Tuple[np.ndarray] | None = None` - валидацонная выборка, на которой будем проверять переобучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aklJJMFOoUyV"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9ZsXqV6oUyW"
      },
      "source": [
        "### Бонус. DART (1.5 балла)\n",
        "\n",
        "Ранее мы обсуждали, что каждый базовый алгоритм пытается исправить ошибки всего предыдущего ансамбля. Из-за этого возникает неравноправие - каждая новая модель вносит все меньший и меньший вклад. Уверенные обучатели нейросетей знают, что это может привести к переобучению. Но выход есть, и выход этот [DART](https://arxiv.org/pdf/1505.01866)!\n",
        "\n",
        "В DL есть такой вид регуляризации, как dropout, когда некоторые узлы нейросети со случайным шансом отключаются. Можно попробовать обобщить этот подход на случай бустингов и случайно выкидывать деревья из композиции во время обучения\n",
        "\n",
        "1. Выбираем, какие деревья убрать. При подсчете антиградиента берем не весь ансамбль, а только ту часть, что осталась\n",
        "2. Домножаем выходы дерева на $1/k$, где $k$ - число удаленных деревьев, чтобы учесть тот факт, что ансамбль был не весь. Тогда вклад отдельного дерева будет не настолько большим\n",
        "3. Домножаем выкинутые деревья на $k/(k+1)$, чтобы масштабы выходов примерно совпадали\n",
        "\n",
        "| <font color='lightblue'>XGBoost</font> | <font color='lightgreen'>LightGBM</font> | <font color='palegoldenrod'>CatBoost</font> |\n",
        "| --- | --- | --- |\n",
        "| `XGBClassifier(booster=\"dart\")` | `LGBMClassifier(boosting_type=\"dart\")` | - |\n",
        "\n",
        "Параметры, которые нужно добавить:\n",
        "\n",
        "- `dart: bool | None = False` - DART бустинг или нет\n",
        "- `dropout_rate: int | float = 0.1` - доля деревьев, которую выкидываем во время обучения DART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJHEF1odoUyW"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "yGiFCx4-oUyW"
      },
      "source": [
        "------\n",
        "\n",
        "## Задание 3. Оптимизация (3.5 балла + 0.5 бонус)\n",
        "\n",
        "Теперь давайте подумаем, как же правильно применять бустинги. Нас интересует: какие бустинги вообще бывают, как их правильно тюнить, и как интерпретировать\n",
        "\n",
        "Оптимизация моделей с большим числом гиперпараметров это сложное дело. Нетрудно догадаться, что в случае, когда параметров $m$, сложность перебора в самом худшем случае будет порядка $n^m$. У бустингов, хотя на самом деле не только, эта проблема стоит особенно остро. Подумаем, как это делать умнее\n",
        "\n",
        "### 3.1. Знакомство с optuna (1 балл)\n",
        "\n",
        "Для эффективного подбора гиперпараметров существует несколько решений, основанных на байесовской оптимизации. В этом задании предлагается познакомиться с библиотекой [optuna](https://optuna.org/), которая делает перебор гиперпараметров легким и приятным, а также сохраняет всю историю экспериментов в одно место за вас\n",
        "\n",
        "Подберите на валидации оптимальные значения следующих гиперпараметров для вашей реализации бустинга:\n",
        "- `max_depth`\n",
        "- `n_estimators`\n",
        "- `learning_rate`\n",
        "- любые другие параметры, которые вам понравились\n",
        "\n",
        "Заметим, что оптуна это крайне гибкая библиотека, в которой можно реализовать абсолютно любую логику, которую пожелаете. За вас там есть лишь выбор модели. Чтобы не плодить несколько `objective`, можно слегка менять тело функции, например, добавить опциональные параметры, но шаблоном пользоваться не обязательно, как обычно\n",
        "\n",
        "Подберите диапазон параметров самостоятельно, не забывайте фиксировать сид (нет, его не нужно перебирать). Сохраните и покажите оптимальный набор параметров. Сравните графики функций потерь для оптимального и дефолтного бустингов. Покажите, какие параметры дали наибольший прирост (для этого придется почитать документацию оптуны)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "wAaz6-fFoUyW"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚\n",
        "\n",
        "def objective(study, model=\"custom_boosting\"):\n",
        "    # параметры, общие для всех бустингов\n",
        "    common_params = ...\n",
        "    # параметры для конкретной имплементации бустинга\n",
        "    optional_params = ...\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HgegfY_oUyW"
      },
      "source": [
        "### 3.2. Глубокая оптимизация (1 балл)\n",
        "\n",
        "Помимо эффективного перебора гиперпараметров, `optuna` предлагает более тонкий контроль над самой процедурой обучения. Улучшите процедуру оптимизации следующим образом:\n",
        "\n",
        "1. Посмотрите, как в `optuna` устроена запись в хранилища, и добавьте это в пайплайн оптимизации. Без указания БД все запуски оптуны сотрутся, как только закончится сессия. Более того, так можно распараллелить процесс оптимизации на несколько машин\n",
        "2. Посмотрите, как можно контролировать процесс обучения бустинга при помощи прунера. У библиотечных имплементаций есть уже готовые, их можно использовать пунктом ниже. Для кастомной модели возьмите что-то из `optuna.pruners`. Идея прунера - не обучать бустинг до конца, если по истории видно, что другие бустинги на этом этапе статистически лучше.\n",
        "3. Добавьте сохранение истории функции потерь и времени обучения в качестве `user_attribute` сразу в `study`. Мы не хотим оптимизировать их напрямую, но они нам понадобится для сравнения моделей в дальнейшем\n",
        "\n",
        "Добавьте хранилище и юзер атрибуты и посмотрите, ускорилась ли оптимизация с добавлением прунера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUHcAqkioUyW"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5otyFtBkoUyW"
      },
      "source": [
        "### 3.3. Готовые реализации (1 балл)\n",
        "\n",
        "Библиотек для бустингов существует много, но активно используются лишь три. Ваша задача - попробовать их в деле, посмотреть на их уникальные фишки и понять, что вам нравится больше всего\n",
        "\n",
        "Возьмите любую из популярных библиотек: <font color='lightblue'>`xgboost`</font>, <font color='lightgreen'>`lightgbm`</font>, <font color='palegoldenrod'>`catboost`</font>.\n",
        "Ваша задача - взять одну из них, подобрать те же оптимальные параметры, что и выше, плюс что-то, что вам понравилось в них больше всего (может быть абсолютно любая фишка из пунктов выше или что-то, что вы найдете сами, не обязательно эксклюзивное), и сравнить две оптимизированные модели по\n",
        "- динамике функции потерь\n",
        "- времени обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEaspH2NoUyW"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PVp7CgjoUyX"
      },
      "source": [
        "<font color='plum'>**Вопрос:** Какая архитектура вам нравится больше всего и почему?</font>\n",
        "\n",
        "<font color='plum'>**Ответ**: ... </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSOceQUzoUyX"
      },
      "source": [
        "### Бонус. Random Forest (0.5 балла)\n",
        "\n",
        "Если очень захотеть, оптуна может оптимизировать все, что угодно, даже нейросети. Давайте противопоставим бустингу его злейшего врага - алгоритм Random Forest.\n",
        "\n",
        "Возьмите реализацию случайного леса из <font color='lightgreen'>`lightgbm`</font>. Посмотрите, какие есть специальные для леса гиперпараметры, переберите их вместе с `max_depth` и `n_estimators`.\n",
        "\n",
        "Сравните его качество и динамику функции потерь с таковыми у бустинга и сделайте вывод, а нужен ли Random Forest?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgVsaIZXoUya"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4_dPeCFvoUya"
      },
      "source": [
        "### 3.4. Бустинг и вероятности (0.5 балла)\n",
        "\n",
        "С качеством мы более-менее разобрались, но что насчет вероятностей? Постройте калибровочную кривую для вашей лучшей модели бустинга. Насколько хорошо бустинг оценивает вероятности?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L7G-nV7ioUya"
      },
      "outputs": [],
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Постройте также калибровочную кривую для логистической регрессии, сравните их между собой. Проанализируйте полученные результаты."
      ],
      "metadata": {
        "id": "-_bQbOIdw9Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ],
      "metadata": {
        "id": "gZq3sd3Lw-R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uFVT_nokoUyb"
      },
      "source": [
        "## Социализационный бонус. Новогоднее 🎆 (0.5 балла)\n",
        "\n",
        "Сфотографируйтесь с наряженной новогодней или рождественской ёлкой! Приложите фотографию, опишите свои впечатления, чего вы ждете от нового 2025 года?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Ur9BXYoUyb"
      },
      "source": [
        "# *𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚*𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚*𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚*𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚*𖣂♱𖠰𖣂↟*𖠰ᨒ↟*𖥧˚"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "210px",
        "width": "492px"
      },
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}